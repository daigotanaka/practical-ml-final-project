---
title: Classification of Weight Lifting Exercises
author: "Daigo Tanaka"
date: "February 20, 2015"
output: html_document
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# My usual front matter

message(paste("Working directory:", getwd(), sep=" "))

library(knitr)
library(RCurl)
library(randomForest)
library(ggplot2)

version = sessionInfo()$R.version$version.string
platform = sessionInfo()$platform

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, comment=NA,
              results="asis", tidy=FALSE, cache=FALSE)

# Set significant digits
options(scipen = 20, digits = 2)

# Load caption helper
code = getURL("https://gist.githubusercontent.com/daigotanaka/17930c2ff891e05a83f5/raw/7b18cf743cc776b0e82f6d3605f194e7143b031f/rmd_caption_helper.R")
eval(parse(text=code))
```

```{r}
# Analysis (Make sure to cache)

# Set seed of pseudo random number generation for reproducibility
set.seed(1254)

# Load datasets
url = getURL(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
training = read.csv(text=url, head=TRUE, na.strings=c("", "NA", "NULL"))

url = getURL(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
testing = read.csv(text=url, head=TRUE, na.strings=c("", "NA", "NULL"))

# Exploratory data analysis
columnNames = names(training)

naCounts = c()
for (i in 1:ncol(training)) {
    naCounts[i] = length(training[is.na(training[, i]), i])
}
naCounts = naCounts[order(-naCounts)]

# Do not use columns with > 50% NAs
cutOff = nrow(training) * 0.5
colsUsed = c()
for (i in 1:ncol(training)) {
    colsUsed[i] = (
        i == ncol(training) ||  # classe
        length(training[!is.na(training[, i]), i]) >= cutOff &&
        (class(training[1, i]) != "factor" || levels(training[1, i]) < 53))
}

# It looks like col X is IDs and classe is sorted that way so let's not use this
# as predictor
colsUsed[1] <- FALSE

# Obviously use_name does nothing to do with the classifciation
colsUsed[2] <- FALSE

# Nor do time stamps...
colsUsed[3] <- FALSE
colsUsed[4] <- FALSE
colsUsed[5] <- FALSE

# What are new_window & num_window?
colsUsed[6] <- FALSE
colsUsed[7] <- FALSE

# length(subset(colsUsed, colsUsed == TRUE))

trainingUsed = training[, colsUsed]

initialModel <- randomForest(
    trainingUsed$classe ~ .,
    data=trainingUsed,
    ntree=10)

# varImpPlot(initialModel)
imp = importance(initialModel)
imp = imp[order(-imp),]

impDf = data.frame(MeanDecreaseGini=imp)
impDf$VariableName = factor(names(imp), levels = rev(names(imp)))
impPlot = ggplot(data=impDf, aes(x=VariableName, y=MeanDecreaseGini)) +
    geom_bar(fill="#FF9999", stat="identity") + coord_flip()

variablesUsed = names(imp)[1:10]

# Do 10 fold cross validation
k = 10

# Assign each observation to one of 10 folds
# Note that this replace=TRUE is NOT replacing the observation when sampling
id <- sample(1:k, nrow(training), replace=TRUE)
list <- 1:k

prediction <- data.frame()
actual <- data.frame()

importantVars = c()
for (i in 1:k){    
    # Create training set from all training date except i-th fold
    currentTraining <- subset(training, id %in% list[-i])
    currentTesting <- subset(training, id %in% c(i))

    currentModel <- randomForest(
        currentTraining$classe ~
            roll_belt +
            yaw_belt +
            magnet_dumbbell_z +
            pitch_forearm +
            magnet_dumbbell_y +
            pitch_belt +
            roll_forearm +
            magnet_dumbbell_x +
            accel_dumbbell_y +
            accel_dumbbell_z,
        data=currentTraining,
        ntree=10)

    message(paste("Model ", i))
    # print(currentModel$confusion)
    
    currentPrediction <- as.data.frame(predict(currentModel, currentTesting))
    prediction <- rbind(prediction, currentPrediction)
    
    currentActual <- as.data.frame(currentTesting$classe)
    actual <- rbind(actual, currentActual)
}

result <- cbind(prediction, actual[, 1])
names(result) <- c("Predicted", "Actual")
confusionMatrix = table(result)
hit = 0
for (i in 1:5) {
    hit = hit + confusionMatrix[i, i]
}
accuracy = hit / sum(confusionMatrix)

# Create the final model using all the training data
buildModel = function() {
    model <- randomForest(
    training$classe ~
        roll_belt +
        yaw_belt +
        magnet_dumbbell_z +
        pitch_forearm +
        magnet_dumbbell_y +
        pitch_belt +
        roll_forearm +
        magnet_dumbbell_x +
        accel_dumbbell_y +
        accel_dumbbell_z,
    data=training,
    ntree=10)
    return (model)
}
finalModel = buildModel()
systemTime = system.time(buildModel())

finalConfusionMatrix = finalModel$confusion[1:5,1:5]
hit = 0
for (i in 1:5) {
    hit = hit + finalConfusionMatrix[i, i]
}
finalInSampleAccuracy = hit / sum(finalConfusionMatrix)

# Moment of truth with the testing data
finalPrediction = as.character(predict(finalModel, testing))

# This should produce...
# "B" "A" "B" "A" "A" "E" "D" "B" "A" "A" "B" "C" "B" "A" "E" "E" "A" "B" "B" "B"
```

This report
```{r}
footnote("source")
```
describes the steps I took to classify "Weight Lifting Exercise
Dataset" generated by Velloso, et al.
```{r}
footnote("Velloso2013")
```
The dataset records the measurements from the sensors attached to the glove,
armband, lumbar belt and dumbbell while the participants lift the dumbbell in
5 different ways: One exactly according to the specification and 4 representing
the common mistakes.

Given a training dataset, the goal is to generate a machine learning model to
accurately classify the separate test dataset into the 5 patterns. This
classification exercise was done in part to complete coursera Data Science
Specialization track. The assignment challenges the students to classify the
final test data of 20 observations, and the model presented in the report
achieved 100% accuracy.

## Data

The training data file
```{r}
footnote("dataset")
```
has `r nrow(training)` observations from `r length(levels(training$user_name))`
participants, recording 10 repetitions of the unilateral dumbbell biceps curl.
Each weight lifting activity is labeled as Class A (according to the
specification), Class B (throwing the elbows to the front), Class C (lifting the
dumbbell only halfway), Class D (lowering the dumbbell only halfway), or Classs
E(throwing the hips to the front).

There are `r ncol(training)` columns in the data table, however;
`r length(naCounts[naCounts < 1])` of them contained N/A only.
Besides, the fist 7 columns (`r paste(columnNames[1:7], collapse=", ")`) are
IDs for users and observations, time stamps, or window information that should
not be used to classify. Those columns are excluded from the predictors.

## Building the model

With the objective is to build a model with sufficient accuracy to correctly
classify the test data (20 cases). Random Forest with 10 trees was chosen as the
initial attempt. Due to the memory implications, a decision was made to drop the
columns that has more than 53 levels of factors.
```{r}
footnote("factors")
```

Total of `r ncol(trainingUsed) - 1` predictors were used to create the initial
model. Figure `r fn()` shows the mean decrease of Gini impurity for each
variable. It can be observed from the plot that at most 20% of the variables are
significantly contributing to decrease the impurity. To save the processing
load, the 10 most important variables are used to make the final model. The
variables used in the final model are
`r paste(variablesUsed[1:9], collapse=", ")`, and `r variablesUsed[10]`.

```{r, html.cap=fn("The Random Forest predictors sorted by the importance measured by mean decrease of Gini impurity.")}
impPlot
```

## Cross-validation

A K-fold cross validation (k=10) was performed to measure the accuracy of the
model. Table `r tn()` shows the confusion matrix from the K-fold cross
validation. The overall accuracy was `r 100 * accuracy`% (or estimated error
rate of `r 100 * (1 - accuracy)`%).

`r render_caption(tn("Confusion matrix from the K-fold Cross Validation (k=10)"))`

|              | Actual: A | B    | C    | D    | E    |
| -----------: | --------: | ---: | ---: | ---: | ---: |
| **Predicted: A** | `r confusionMatrix[1,1]` | `r confusionMatrix[1,2]` | `r confusionMatrix[1,3]` | `r confusionMatrix[1,4]` | `r confusionMatrix[1,5]` |
| **B**            | `r confusionMatrix[2,1]` | `r confusionMatrix[2,2]` | `r confusionMatrix[2,3]` | `r confusionMatrix[2,4]` | `r confusionMatrix[2,5]` |
| **C**            | `r confusionMatrix[3,1]` | `r confusionMatrix[3,2]` | `r confusionMatrix[3,3]` | `r confusionMatrix[3,4]` | `r confusionMatrix[3,5]` |
| **D**            | `r confusionMatrix[4,1]` | `r confusionMatrix[4,2]` | `r confusionMatrix[4,3]` | `r confusionMatrix[4,4]` | `r confusionMatrix[4,5]` |
| **E**            | `r confusionMatrix[5,1]` | `r confusionMatrix[5,2]` | `r confusionMatrix[5,3]` | `r confusionMatrix[5,4]` | `r confusionMatrix[5,5]` |


### Expected out of sample error

The high accuracy result from K-fold cross validation did not require further
search of the models. Using the 10 chosen predictors, the final model was created
from the entire training data set.

The in-sample accuracy of the final model was `r 100 * finalInSampleAccuracy`%.
So the out of sample error is expected to be higher than
`r 100 * (1.0 - finalInSampleAccuracy)`%. The accuracy seemed to be promising
for this model to be used to classify the test data set.

With the order of magnitude smaller number of predictors than the initial model,
it took only `r systemTime[3]` seconds of elapsed time to build the model.

## Result with the test data

The classification result of the testing data set
```{r}
footnote("submission")
```
was `r paste(finalPrediction[1:19], collapse=", ")`, and `r finalPrediction[20]`
for the 20 cases in the order, and they were validated to be all correct.

```{r}
footnote_labels = c("source", "Velloso2013", "dataset", "factors", "submission")
footnote_contents = c(
    paste('This R markdown document was processed with ', version, ' on ', platform, '. The R source code produced the analysis and this report is available from <a href="https://github.com/daigotanaka/practical-ml-final-project/blame/master/practical-ml-final-project.Rmd">github page</a>', sep=""),
    "Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.",
    "A dataset was created for the Coursera Practical Machine Learning class by modifying the original dataset.",
    "randomForest package of R cannot handle the factor variable with over 53 levels.",
    "The actual Classes were not known before the submission of the model prediction to the course evaluation server.")
footnotes = data.frame(label=footnote_labels, content=footnote_contents)
renderFootNotes(
    footnotes,
    head="<h3>References and notes</h3>")
```

```{r}
# Produce the submission file
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = sprintf("problem_id_%02d.txt",i)
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(finalPrediction)
```